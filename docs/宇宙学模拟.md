# 宇宙学模拟

## 什么是宇宙学模拟，我们为何要模拟宇宙？

在人类探索宇宙的宏伟历程中，我们对宇宙的理解经历了漫长的过程. 传统的宇宙学研究依赖于观测和理论，观测揭示宇宙的现象，而理论则试图解释这些现象背后的物理原理. 然而，随着我们对宇宙的理解和认识越来越深入，一个巨大的鸿沟出现了，当理论变得极其复杂且非线性（比如广义相对论），而研究的对象又是一个充满复杂细节的真实宇宙时，理论与观测之间出现了巨大的鸿沟. 宇宙学家可以通过均匀且各向同性的假设来描述一个理想化的宇宙，但要求解出成千上万个星系在引力作用下如何相互作用、演化成我们今天宇宙的大尺度结构，是人力所无法完成的.

为了跨越这一鸿沟，计算作为第三大支柱应运而生，并迅速成为现代宇宙学不可或缺的组成部分. 宇宙学模拟的目标是，基于现有的宇宙学理论，从天文观测得到的早期宇宙的数据出发（比如从 CMB 观测得到的功率谱），通过计算机模拟宇宙的演化过程，并和观测数据进行比较，从而验证现有宇宙学理论的正确性，并揭示宇宙的演化规律. 

宇宙学模拟的发展历史一方面同天文学理论的进步密切关联，另一方面也与计算能力的指数级增长密不可分. 但我们对模拟宇宙的尝试，并非始于计算机程序，而是始于灯泡和光. 1941 年，在世界上第一台电子计算机 ENIAC 诞生之前，瑞典天文学家埃里克·霍姆伯格就开始了宇宙学模拟的尝试，他巧妙地应用光强和引力都遵循相同的平方反比定律这一共性，在实验室中利用灯泡来模拟星体之间引力，通过改变灯泡的电压来反映星体的质量，并通过一个具有余弦矫正的光电管来测量光的强度，从而模拟出星体所受到的引力，并以此计算出下一个时刻的位置和速度. 他采用 74 个灯泡来模拟两个星系的碰撞过程. 并以此研究星系碰撞的轨道能量损失，这个过程虽然艰辛，但却是历史上第一次宇宙学模拟.

随着电子计算机的发展，到 1960 年代，天体物理学家开始利用新兴的电子计算机来研究多体系统在引力作用下的演化过程. 此时的模拟大多采用基于 N 体引力的模拟，即在模拟中宇宙的物质由 N 个粒子所替代，粒子与粒子之间遵循牛顿引力定律. 此时的计算机性能较差，且算法的复杂度较高，能模拟的粒子数量较低. 即使有如此限制，宇宙学模拟也对理论的发展起到了一定的作用. 

20 世纪 70 年代，美国天文学家薇拉·鲁宾（Vera Rubin）及其合作者通过对螺旋星系进行观测发现，星系外缘恒星的旋转速度并不会像预期的那样随半径增大而减小，反而是保持在一个几乎恒定的水平上，从而表明星系被包裹在一个巨大的、延伸范围远超其发光盘面的不可见物质晕之中. 与此同时，宇宙学模拟也为暗物质晕的存在提供了另一块关键的基石，在1973年发表的一篇里程碑式的论文中，普林斯顿大学的耶利米·奥斯特里克（Jeremiah Ostriker）和詹姆斯·皮布尔斯（James Peebles）利用早期的 N 体模拟对孤立的星系盘进行了稳定性分析. 他们的计算表明，一个纯由恒星组成的引力盘是极端不稳定的，会在很短的时间内形成一个棒状结构，或者瓦解 . 但如果在星系盘的外部包裹一个质量巨大的、大致呈球形的暗物质晕，其引力作用便可以有效地抑制这种不稳定性. 这些来自观测和计算的独立证据使得暗物质的存在从一个边缘猜想转变为 70 年代末天体物理学家普遍接受的核心概念.

20 世纪 70 年代末至 80 年代宇宙学模拟迎来了异常深刻的算法革命，两种高效的引力计算方法：粒子网格法以及层次树法被引入宇宙学模拟，使得更大规模的模拟成为可能. 粒子网格法巧妙地将描述引力势的泊松方程的求解通过快速傅里叶变换转成了一个简单的代数方程，从而直接给出引力势能. 

层次树则将粒子进行分类，在计算某个特定粒子受到一群距离较远的粒子的引力作用时，直接将较远距离的粒子群当成整体，只考虑它的低阶矩的影响，而不去计算里面的每一个粒子的具体影响. 以此大大减少了模拟所需的计算量. 

借助两种新的方法，以及计算机算力的发展，使得宇宙学模拟从之前的模拟几个星系之间的相互作用，到模拟更大尺度的宇宙的演化过程成为可能. 1985年，戴维斯、埃夫斯塔希欧、弗伦克和怀特（DEFW）在《天体物理学杂志》上发表的论文，标志着宇宙学模拟进入了一个新时代. 这是首批专门为严格检验冷暗物质模型而设计的大规模宇宙学模拟之一，借助粒子网格法，团队模拟了超 3 万个暗物质粒子，展示了暗物质粒子从近乎均匀的初始分布，逐渐演化成一个复杂的，具有纤维状结构的过程. 但模拟给出的暗物质聚集程度低于当时 CfA 巡天观测的聚集程度，面对这一问题，他们提出了一个极具洞察力的解决方案：星系形成是有偏袒性的，也就是星系只在那些暗物质密度足够高的区域才能形成. 在修正后，模拟和巡天观测结果惊人地吻合. 从而巩固了冷暗物质在宇宙学模型中的重要地位. “偏袒性”在宇宙学的引入也证实了宇宙学模拟不止是一个验证工具，也具有强大的预测和发现能力.

进入 21 世纪，随着计算能力的飞速增长，宇宙学模拟的尺度和精度也在飞速增长. 最终在 2005 年达到了一个里程碑：千禧年模拟，这次模拟追踪了超过 100 亿个粒子，超级计算机运行了一个多月，并产生了 25TB 的数据. 千禧年模拟的成功展示了模拟宇宙暗物质“骨架”的力量，但 N 体引力模拟只模拟了引力的作用，并没有模拟恒星形成等复杂的重子物理过程. 2010 年，新一代的基于“流体力学”的宇宙学模拟兴起，2014 年的 Illustris 和 2015 年的 EAGLE 等项目



当然，宇宙学模拟在今天仍旧面临各种挑战. 一方面，


---

第一批数字计算机的到来开启了宇宙学模拟的一个时代，在 1960 年代，德国的塞巴斯蒂安·冯·赫纳和剑桥大学的斯韦勒·阿瑟斯等人进行了首次纯计算的 N 体模拟，但受限于此时计算机的性能和算法，此时这些都是规模不大的尝试，只追踪几十到大约100个粒子. 到了 70 年代，

吉姆·皮布尔斯（Jim Peebles）和西蒙·怀特（Simon White）等研究人员开始使用数百个粒子的模拟来模拟星系团的引力坍缩。 1974年，威廉·H·普雷斯（William H. Press）和保罗·谢克特（Paul Schechter）使用模拟来探索他们关于宇宙结构质量分布如何源于等级聚类（即小天体先形成然后合并成更大天体）的有影响力的理论，这标志着向真正宇宙学模拟的观念飞跃.

80 年代，随着（），冷暗物质理论提出，宇宙中充满了不可见的物质，正是它塑造了宇宙，这给宇宙学模拟提出了一个明确的问题：一个充满了不可见的冷暗物质的宇航走，是否能创造出我们宇宙的大尺度结构？

计算能力开始追赶上理论的雄心. 1981年，首次发布了超过20,000个粒子的模拟，对宇宙结构做出了可信的预测. 1985年，马克·戴维斯（Marc Davis）、乔治·埃夫斯塔西欧（George Efstathiou）、卡洛斯·弗伦克（Carlos Frenk）和西蒙·怀特（通常缩写为DEFW）进行了一次数值为32,768个粒子的模拟，这是一个里程碑式的时刻. 他们的结果表明，一个由 CDM 主导的宇宙确实可以产生与观测结果非常相似的大尺度结构，从而巩固了ΛCDM作为标准宇宙学模型的地位.

随着计算能力的飞速增长，宇宙学模拟中的粒子数量也不断增加，最终在 2005 年达到了一个令人惊叹的千禧年模拟，这次模拟追踪了超过 100 亿个例子，超级计算机运行了一个多月，并产生了 25TB 的数据. 

千禧年模拟的成功展示了模拟宇宙暗物质“骨架”的力量。下一个巨大挑战是添加血肉：构成恒星和星系的普通重子物质。2010年代见证了新一代“流体动力学”模拟的兴起，这些模拟在引力之外还模拟了宇宙气体的复杂物理过程。

这一进展是由观测、理论和计算之间的共生关系驱动的。例如，千禧年模拟使用的宇宙学参数后来被威尔金森微波各向异性探测器（WMAP）卫星更精确的测量所取代。这一观测进展直接推动了2010年的Bolshoi模拟，该模拟使用这些更新、更准确的参数重新运行了一个大尺度宇宙。   

2014年的Illustris和2015年的EAGLE等里程碑式项目，是首批通过包含流体动力学和复杂的“次网格”模型（用于模拟恒星形成和黑洞反馈等过程），在大型宇宙学代表性体积内产生逼真星系群的项目。 最新一代，如IllustrisTNG，通过纳入磁场效应（磁流体动力学）并探索不同体积和分辨率的嵌套模拟套件（TNG50、TNG100和TNG300），进一步推动了边界。 今天，有效粒子数达到数万亿的模拟正以前所未有的保真度探测宇宙，这是该领域先驱们无法想象的进步规模。






从巧妙的桌面装置到遍布全球的超级计算机网络，构建虚拟宇宙的追求在80多年的时间里不断推动着科学和技术的边界



## 未来和挑战
